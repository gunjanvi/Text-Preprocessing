{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Text Mining</center>\n",
    "\n",
    "___\n",
    "\n",
    "Text data falls into the category of unstructured data and requires some preparation before it can be used for modeling. Text preperation is different from structed data pre-processing.\n",
    "\n",
    "Today we will go through the process of preparing text data and building a predictive model on it.\n",
    "\n",
    "## Why SpaCy?\n",
    "\n",
    "There are many different libraries that can be used for text related processing. We will work with SpaCy.\n",
    "\n",
    ">SpaCy is a free and open-source library developed by Explosion AI. It works well for simple to complex language understanding tasks and is designed specifically for production use.\n",
    "\n",
    "SpaCy provides trained models for 48 differnt languages and has a model for multi-language as well. \n",
    "\n",
    ">Check this link for various English models : https://spacy.io/models/en\n",
    "\n",
    "Before jumping in, let's have a look at various features provided by popular NLP related libraries and their performance in compairision to SpaCy.  \n",
    "_*All charts are referenced from SpaCy Docs*_\n",
    "\n",
    "### Feature Comparision\n",
    "\n",
    "![](img/feature-comparision.png)\n",
    "\n",
    "### Speed Comparision\n",
    "\n",
    "![](img/speed-comparision.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SpaCy Installation\n",
    "\n",
    "To get started with SpaCy, install the package using pip in Terminal (for Mac) or CommandLine (for Windows)\n",
    "\n",
    "The language pre-trained model packages can be downloaded using the \"spacy download\" command. We will download `en_core_web_md` package\n",
    "\n",
    "- en = English\n",
    "- core = Core (Vocab, Syntax, Entities, Vectors)\n",
    "- web = Web Text\n",
    "- sm/md/lg = Small/Medium/Large"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!pip install spacy\n",
    "!python -m spacy download en_core_web_md"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify the library is installed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#! pip list | grep spacy\n",
    "#! pip list | grep en-core-web-sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import SpaCy Module and Load the Language Model\n",
    "\n",
    "*We are loading Medium model as Small model doesn't contain word vectors that we will use later. If you are not using word vectors, you can load Small model as well.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello World\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Load the language model\n",
    "nlp = spacy.load('en_core_web_md')\n",
    "\n",
    "# Create SpaCy Object\n",
    "doc = nlp(\"Hello World\")\n",
    "\n",
    "# Print the document text\n",
    "print(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Preprocessing\n",
    "\n",
    "Basic steps in text pre-processing are :\n",
    "\n",
    "- Tokenisation \n",
    "- Stop Words removal\n",
    "- Matcher and PhraseMatcher\n",
    "- Lemmatization\n",
    "- Vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding SpaCy Objects\n",
    "\n",
    "#### I. `nlp` Object\n",
    "\n",
    "When we load the SpaCy model, it creates the SpaCy object. We define it with the variable name `nlp`. \n",
    "\n",
    "This object contains the language specific vocabulary, model weights and processing pipeline like tokenisation rules, stop words, POS rules etc.\n",
    "\n",
    "![](img/nlp.png)\n",
    "\n",
    "Look for pipeline component names using `pipe_names` attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tagger', 'parser', 'ner']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('tagger', <spacy.pipeline.pipes.Tagger at 0x117d65c50>),\n",
       " ('parser', <spacy.pipeline.pipes.DependencyParser at 0x119741d08>),\n",
       " ('ner', <spacy.pipeline.pipes.EntityRecognizer at 0x119741d68>)]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(nlp.pipe_names)\n",
    "\n",
    "nlp.pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we process some text with `nlp` object, it creates a doc object, short for document.\n",
    "\n",
    "#### II. `Doc` Object\n",
    "\n",
    "![](img/doc.png)\n",
    "\n",
    "Token objects represent the word tokens in the document. To get a token at a specific position, simply index the Doc object like any python object.\n",
    "\n",
    "#### III. `Span` Object\n",
    "\n",
    "![](img/span.png)\n",
    "\n",
    "A Span object is a slice of the document consisting of one or more tokens. Again to view a span, simply index with start and end position seperated by : like any python object.\n",
    "\n",
    "## Tokenisation\n",
    "\n",
    "#### I. Word Tokenisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc Object   :  Have a Good Day!\n",
      "Token Object :  a\n",
      "Token Object :  !\n",
      "Span Object  :  a Good Day\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"Have a Good Day!\")\n",
    "\n",
    "# Print the document text\n",
    "print(\"Doc Object   : \", doc.text)\n",
    "\n",
    "# Get the token text using .text attribute\n",
    "print(\"Token Object : \", doc[1].text) \n",
    "print(\"Token Object : \", doc[-1].text) # punctuation is also a token\n",
    "\n",
    "# Take a span of tokens\n",
    "span = doc[1:4]\n",
    "\n",
    "# Get the span text using .text attribute\n",
    "print(\"Span Object  : \", span.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In case all you want to do is tokenise and don't want to initialise rest of the pipeline. There are two options.\n",
    "\n",
    "Let's compare them using the pos_ attribute on the token and check the output with only tokeniser and with complete pipeline.\n",
    "\n",
    "- Default Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello world!\n",
      "NOUN\n"
     ]
    }
   ],
   "source": [
    "# Process the text \n",
    "doc = nlp(\"Hello world!\")\n",
    "\n",
    "print(doc.text)\n",
    "print(doc[1].pos_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## POS Tags\n",
    "* ADJ: adjective\n",
    "* ADP: adposition\n",
    "* ADV: adverb\n",
    "* AUX: auxiliary verb\n",
    "* CONJ: coordinating conjunction\n",
    "* DET: determiner\n",
    "* INTJ: interjection\n",
    "* NOUN: noun\n",
    "* NUM: numeral\n",
    "* PART: particle\n",
    "* PRON: pronoun\n",
    "* PROPN: proper noun\n",
    "* PUNCT: punctuation\n",
    "* SCONJ: subordinating conjunction\n",
    "* SYM: symbol\n",
    "* VERB: verb\n",
    "* X: other"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Doc object Only\n",
    "\n",
    "`make_doc( )` function creates a doc object with only tokeniser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello world!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Process the text \n",
    "doc = nlp.make_doc(\"Hello world!\")\n",
    "\n",
    "print(doc.text)\n",
    "print(doc[1].pos_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Tokeniser object ( Temporarily )\n",
    "\n",
    "Disable remaining pipeline in with statement. This doc is not available outside with statement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Disable tagger and parser\n",
    "with nlp.disable_pipes('tagger', 'parser'):\n",
    "    \n",
    "    # Process the text \n",
    "    doc = nlp(\"Hello world!\")\n",
    "    print(doc[1].pos_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### II. Sentence Tokenisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Have a Good Day!', 'Same to you.']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = nlp(\"Have a Good Day! Same to you.\")\n",
    "\n",
    "[w.text for w in doc.sents]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Custom Pipeline Component\n",
    "\n",
    "Let's see how to add a custom component to the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tagger', 'parser', 'ner', 'size_info']\n",
      "This doc has 4 tokens.\n",
      "This is a small review\n",
      "This doc has 11 tokens.\n",
      "This is a long review\n"
     ]
    }
   ],
   "source": [
    "nlp1 = spacy.load('en_core_web_sm')\n",
    "\n",
    "\n",
    "def add_comp1(doc):\n",
    "    \n",
    "    print(\"This doc has {} tokens.\".format(len(doc)))\n",
    "    \n",
    "    if len(doc) < 10 :\n",
    "        print(\"This is a small review\")\n",
    "    elif len(doc) > 10 :\n",
    "        print(\"This is a long review\")\n",
    "        \n",
    "    return doc\n",
    "\n",
    "nlp1.add_pipe(add_comp1, name = \"size_info\", last = True)\n",
    "\n",
    "print(nlp1.pipe_names)  \n",
    "\n",
    "doc = nlp1(\"The moview was good\")\n",
    "\n",
    "doc = nlp1(\"I loved the movie. It was a great experience!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STOP WORDS\n",
    "\n",
    "Words that occur very frequently in the documents that they don't add any meaning or value are called stop words. It best to remove these words that are useless and consume a lot of resources.\n",
    "\n",
    "##### Remember : Stop words are language & domain dependent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['after', 'becomes', 'indeed', 'becoming', 'at', 'mine', 'without', 'cannot', 'less', 'call']\n",
      "['Hi', 'Bye', 'after', 'becomes', 'indeed', 'becoming', 'at', 'mine', 'without', 'cannot']\n",
      "['no', 'than', 'third', 'after', 'meanwhile', 'his', 'becomes', 'indeed', 'once', 'becoming']\n"
     ]
    }
   ],
   "source": [
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "\n",
    "# View the default stop words list\n",
    "print(list(STOP_WORDS)[:10])\n",
    "\n",
    "\n",
    "# Add some stop words to default list\n",
    "stopwords = ['Hi', 'Bye'] + list(STOP_WORDS)\n",
    "print(stopwords[:10])\n",
    "\n",
    "# Remove some stop words to default list\n",
    "stopwords = set(STOP_WORDS) - {'except', 'was'}\n",
    "print(list(stopwords)[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lexical Attributes\n",
    "\n",
    "Attributes that don't hold any contextual information are called lexical attributes. \n",
    "Let's explore other available token attributes :\n",
    "\n",
    "- i - index\n",
    "- text - token text\n",
    "- is_alpha - alphanumeric character (True/False)\n",
    "- is_punct - punctuation (True/False)\n",
    "- like_num - alphanumeric character (True/False)\n",
    "\n",
    "refer to https://github.com/explosion/spaCy/issues/1439 for all available lexical attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index:    [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
      "Text:     ['I', 'was', 'stuck', 'in', 'traffic', 'for', 'two', 'hours', 'and', 'reached', 'home', '@', '8']\n",
      "is_alpha: [True, True, True, True, True, True, True, True, True, True, True, False, False]\n",
      "is_punct: [False, False, False, False, False, False, False, False, False, False, False, True, False]\n",
      "like_num: [False, False, False, False, False, False, True, False, False, False, False, False, True]\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"I was stuck in traffic for two hours and reached home @ 8\")\n",
    "\n",
    "print('Index:   ', [token.i for token in doc])\n",
    "print('Text:    ', [token.text for token in doc])\n",
    "print('is_alpha:', [token.is_alpha for token in doc])\n",
    "print('is_punct:', [token.is_punct for token in doc])\n",
    "print('like_num:', [token.like_num for token in doc]) # observe both 'two' and '8'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatization\n",
    "\n",
    "Lemmas are root form of a word. It is helpful to reduce the bag of words by using the same root word for all similar kind of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('I', '-PRON-'), ('have', 'have'), ('5', '5'), ('friends', 'friend'), ('but', 'but'), ('my', '-PRON-'), ('best', 'good'), ('friend', 'friend'), ('is', 'be'), ('Kathie', 'Kathie')]\n"
     ]
    }
   ],
   "source": [
    "# Process a text\n",
    "doc = nlp(\"I have 5 friends but my best friend is Kathie\")\n",
    "# Print the text and the predicted tags\n",
    "print([(w.text, w.lemma_) for w in doc]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistical Models\n",
    "\n",
    "Now, let us look at some context based attributes. All the information to make these predictions is also loaded with the model. These models are trained on large datasets of labeled example texts. \n",
    "\n",
    "#### I. Part of Speech\n",
    "\n",
    "- pos_ - Part of Speech\n",
    "\n",
    "POS means labeling words in a sentence as nouns, adjectives, verbs, tense etc. This is particularly helpful for identifying homophones in speech to text analysis. _*eg. If you accidentally drank a bottle of fabric dye, you might die.*_ **[ GOOD TO KNOW ðŸ˜€ ]**\n",
    "\n",
    "\n",
    "refer to https://spacy.io/api/annotation#pos-tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('I', 'PRON'), ('am', 'VERB'), ('20', 'NUM'), ('years', 'NOUN'), ('old', 'ADJ')]\n",
      "[('pen', 'VERB'), ('a', 'DET'), ('letter', 'NOUN'), ('to', 'ADP'), ('your', 'DET'), ('pen', 'NOUN'), ('pal', 'NOUN')]\n"
     ]
    }
   ],
   "source": [
    "# Process a text\n",
    "doc = nlp(\"I am 20 years old\")\n",
    "\n",
    "# Print the text and the predicted tags\n",
    "print([(w.text, w.pos_) for w in doc])\n",
    "\n",
    "\n",
    "doc = nlp(\"pen a letter to your pen pal\")\n",
    "\n",
    "print([(w.text, w.pos_) for w in doc])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use spacy.explain( [tag] ) function to find the meaning of the different tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pronoun\n",
      "numeral\n",
      "verb\n",
      "noun\n",
      "adjective\n",
      "adposition\n",
      "determiner\n"
     ]
    }
   ],
   "source": [
    "print(spacy.explain('PRON'))\n",
    "print(spacy.explain('NUM'))\n",
    "print(spacy.explain('VERB'))\n",
    "print(spacy.explain('NOUN'))\n",
    "print(spacy.explain('ADJ'))\n",
    "print(spacy.explain('ADP'))\n",
    "print(spacy.explain('DET'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Determiners** are one of the ingredients of noun phrases that is, determining exactly which of several possible alternative objects in the world is referred to by a noun phrase\n",
    "\n",
    "**Adpositional** phrases contain an adposition (preposition, postposition, or circumposition) as head and usually a complement such as a noun phrase\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dep_ - syntatic dependency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('I', 'nsubj'), ('am', 'ROOT'), ('20', 'nummod'), ('years', 'npadvmod'), ('old', 'acomp')]\n"
     ]
    }
   ],
   "source": [
    "# Process a text\n",
    "doc = nlp(\"I am 20 years old\")\n",
    "\n",
    "# Print the text and the predicted tags\n",
    "print([(w.text, w.dep_) for w in doc])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find Meanings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nominal subject\n",
      "None\n",
      "None\n",
      "noun phrase as adverbial modifier\n",
      "adjectival complement\n"
     ]
    }
   ],
   "source": [
    "print(spacy.explain('nsubj'))\n",
    "print(spacy.explain('ROOT'))\n",
    "print(spacy.explain('nummod'))\n",
    "print(spacy.explain('npadvmod'))\n",
    "print(spacy.explain('acomp'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualise using displacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"862ea1a5fca8408982dad486579e47de-0\" class=\"displacy\" width=\"925\" height=\"312.0\" direction=\"ltr\" style=\"max-width: none; height: 312.0px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">I</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">PRON</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"225\">am</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"225\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"400\">20</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"400\">NUM</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"575\">years</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"575\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"750\">old</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"750\">ADJ</tspan>\n",
       "</text>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-862ea1a5fca8408982dad486579e47de-0-0\" stroke-width=\"2px\" d=\"M70,177.0 C70,89.5 220.0,89.5 220.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-862ea1a5fca8408982dad486579e47de-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M70,179.0 L62,167.0 78,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-862ea1a5fca8408982dad486579e47de-0-1\" stroke-width=\"2px\" d=\"M420,177.0 C420,89.5 570.0,89.5 570.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-862ea1a5fca8408982dad486579e47de-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nummod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M420,179.0 L412,167.0 428,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-862ea1a5fca8408982dad486579e47de-0-2\" stroke-width=\"2px\" d=\"M595,177.0 C595,89.5 745.0,89.5 745.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-862ea1a5fca8408982dad486579e47de-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">npadvmod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M595,179.0 L587,167.0 603,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-862ea1a5fca8408982dad486579e47de-0-3\" stroke-width=\"2px\" d=\"M245,177.0 C245,2.0 750.0,2.0 750.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-862ea1a5fca8408982dad486579e47de-0-3\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">acomp</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M750.0,179.0 L758.0,167.0 742.0,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from spacy import displacy # for visualisation\n",
    "\n",
    "displacy.render(doc, style=\"dep\", jupyter=True)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from spacy import displacy\n",
    "\n",
    "displacy.serve(doc, style=\"dep\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Named Entity Recognition\n",
    "\n",
    "Named entities are \"real world objects\" that are assigned a name, such as people, places, things, locations, currencies, and more.\n",
    "\n",
    "Named entities can be accessed by using `doc.ents` which returns and iterator of span objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Virat Kohli PERSON 0 11\n",
      "India GPE 13 18\n",
      "Sachin PERSON 30 36\n",
      "Lara PERSON 41 45\n",
      "20,000 CARDINAL 66 72\n"
     ]
    }
   ],
   "source": [
    "# Process a text\n",
    "doc = nlp(\"Virat Kohli (India) surpasses Sachin and Lara, becomes fastest to 20,000 international runs.\")\n",
    "\n",
    "# Iterate over the predicted entities\n",
    "\n",
    "for ent in doc.ents:\n",
    "    # Print the entity text and its label\n",
    "    print(ent.text, ent.label_, ent.start_char, ent.end_char)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find Meanings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numerals that do not fall under another type\n",
      "\"first\", \"second\", etc.\n",
      "Countries, cities, states\n"
     ]
    }
   ],
   "source": [
    "print(spacy.explain('CARDINAL'))\n",
    "print(spacy.explain('ORDINAL'))\n",
    "print(spacy.explain('GPE'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    Virat Kohli\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       " (\n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    India\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       ") surpasses \n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    Sachin\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       " and \n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    Lara\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       ", becomes fastest to \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    20,000\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">CARDINAL</span>\n",
       "</mark>\n",
       " international runs.</div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "displacy.render(doc, style=\"ent\", jupyter=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pattern Matching\n",
    "\n",
    "#### I. Token Matcher\n",
    "\n",
    "Similar to regular expressions, `Matcher` module in SpaCy allow pattern matching for Doc and Token objects.\n",
    "\n",
    "A pattern object is a list of dictionaries that can be added to the matcher object using add( ) function.\n",
    "\n",
    "- The first argument is a unique name for the pattern. \n",
    "- The second argument is an optional callback. We don't need it, so we set it to None. \n",
    "- The third argument is the pattern.\n",
    "\n",
    "Matcher returns a list of tuples each consisting \n",
    "\n",
    "- hash value of pattern name\n",
    "- start index\n",
    "- end index\n",
    "\n",
    "Try the make a token optional use `OP` key\n",
    "\n",
    "- `!` - match 0 times (negate)\n",
    "- `?` - match 0 or 1 times (optional)\n",
    "- `+` - match 1 or more times \n",
    "- `*` - match 0 or more times\n",
    "\n",
    "ref: https://spacy.io/usage/rule-based-matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the Matcher\n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "# Initialize the matcher with the shared vocab\n",
    "matcher = Matcher(nlp.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 1. match text as such and case-insensitive\n",
    "pattern_1 = [{'TEXT': 'Virat'}, {'LOWER': 'kohli'}]\n",
    "\n",
    "# 2. match digit\n",
    "pattern_2 = [{'IS_DIGIT': True}]\n",
    "\n",
    "# 3. match lemma and part of speech\n",
    "# the \"?\" operator makes the fast lemma token optional\n",
    "pattern_3 = [{'LEMMA': 'fast', 'OP': '?'}, {'POS': 'ADJ'}, {'POS': 'NOUN'}]\n",
    "\n",
    "# 4. match lemma and part of speech\n",
    "pattern_4 = [{'IS_UPPER': True}]\n",
    "\n",
    "# Add the pattern to the matcher\n",
    "matcher.add('PATTERN_1', None, pattern_1)\n",
    "matcher.add('PATTERN_2', None, pattern_2)\n",
    "matcher.add('PATTERN_3', None, pattern_3)\n",
    "matcher.add('PATTERN_4', None, pattern_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PATTERN_1\n",
      "Virat Kohli\n",
      "PATTERN_4\n",
      "FASTEST\n",
      "PATTERN_3\n",
      "FASTEST Indian cricketer\n",
      "PATTERN_3\n",
      "Indian cricketer\n",
      "PATTERN_2\n",
      "20000\n",
      "PATTERN_3\n",
      "20000 international runs\n",
      "PATTERN_3\n",
      "international runs\n"
     ]
    }
   ],
   "source": [
    "# Process some text\n",
    "doc = nlp(\"Virat Kohli becomes FASTEST Indian cricketer to score 20000 international runs.\")\n",
    "\n",
    "# Call the matcher on the doc\n",
    "matches = matcher(doc)\n",
    "\n",
    "# Iterate over the matches\n",
    "for match_id, start, end in matches:\n",
    "    # Get the matched span\n",
    "    print(nlp.vocab.strings[match_id])\n",
    "    matched_span = doc[start:end]\n",
    "    print(matched_span.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### II. Phrase Matcher\n",
    "\n",
    "In case you want to match a span or phrase, use `PhraseMatcher` and create doc object as patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the Matcher\n",
    "from spacy.matcher import PhraseMatcher\n",
    "\n",
    "# Initialize the matcher with the shared vocab\n",
    "matcher = PhraseMatcher(nlp.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. match text as such and case-insensitive\n",
    "pattern_1 = nlp('Virat Kohli')\n",
    "\n",
    "# 2. match text as such and case-insensitive\n",
    "pattern_2 = nlp('20000')\n",
    "\n",
    "# Add the pattern to the matcher\n",
    "matcher.add('NAME', None, pattern_1)\n",
    "matcher.add('RUNS', None, pattern_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Virat Kohli\n",
      "20000\n"
     ]
    }
   ],
   "source": [
    "# Process some text\n",
    "doc = nlp(\"Virat Kohli becomes fastest Indian cricketer to score 20000 international runs.\")\n",
    "\n",
    "# Iterate over the matches\n",
    "for match_id, start, end in matcher(doc):\n",
    "    \n",
    "    # Get the matched span\n",
    "    span = doc[start:end]\n",
    "    print(span.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similarity\n",
    "\n",
    "When we load the language model, it also loads the 300-dimensional vector representation for the words. The vector representation has been computed using the Word2Vec algorithm on large Web text.\n",
    "\n",
    "*You will learn more about Word2Vec in Deep Learning Module.*\n",
    "\n",
    "*Its important to note that the small model doesn't contatin word vectors. That is also the reason it loads faster.*\n",
    "\n",
    "SpaCy computes a similarity score between 0-1 between these vectors, at 3 levels \n",
    "- Doc,\n",
    "- Span, and\n",
    "- Token, \n",
    "\n",
    "using the similarity( ) function. By default, it uses cosine similarity.\n",
    "\n",
    "*Similarity is averaged for Doc and Span.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2.9832e-01  1.7471e-01  7.0592e-02  5.1045e-01  2.3978e-01  7.5919e-02\n",
      "  2.3031e-01 -6.1956e-01  2.1394e-01 -1.1881e+00 -1.7273e-01 -3.8365e-01\n",
      " -8.6379e-01  2.4508e-02  9.3082e-02  4.1044e-01 -2.0216e-01 -1.3796e+00\n",
      "  4.1507e-01  5.9532e-01  1.7056e-01  5.0616e-01  4.3366e-01  4.9059e-01\n",
      " -3.0667e-01  4.5887e-01  1.7847e-02  7.8978e-01 -3.6883e-01  5.2176e-01\n",
      "  3.1528e-01 -9.4568e-03 -1.0214e-01  3.8308e-01 -7.4116e-01  2.5485e-01\n",
      " -3.8609e-01 -1.3838e-01  9.9488e-02 -4.1722e-01  4.0797e-01 -5.3534e-01\n",
      " -3.1326e-01 -1.7172e-01 -4.0924e-01  3.3197e-01 -1.9134e-01  1.1649e-02\n",
      "  1.6144e-01 -2.4808e-01  4.0267e-01  2.9613e-01  6.2917e-02 -2.6367e-01\n",
      " -4.7752e-01  1.1263e-02 -1.2182e-01  3.3558e-01 -4.6654e-01 -1.8428e-01\n",
      "  2.5530e-02 -1.4487e-01 -4.4719e-02 -2.5467e-02 -7.3004e-02  5.4738e-01\n",
      "  7.8649e-02  2.9088e-01  1.5538e-01 -4.4414e-01  2.1993e-03 -2.1081e-01\n",
      "  1.5998e-01 -7.3798e-01  2.2719e-01 -7.2435e-01 -5.0498e-01  4.7991e-02\n",
      "  5.4960e-01 -2.2687e-01  4.4384e-01 -1.9989e-01  2.6162e-01  7.0157e-01\n",
      " -1.0815e-01 -1.9819e-01  1.1684e+00 -3.4274e-01 -1.0901e-01 -4.8786e-01\n",
      " -3.4872e-01 -9.5544e-02 -6.2328e-01 -2.7593e-01  5.7172e-01  3.2734e-01\n",
      "  9.3017e-02 -4.1861e-01 -5.2112e-01 -3.6743e-01  2.5636e-01 -7.5818e-01\n",
      " -3.4817e-01 -1.9175e-01  6.1267e-03 -1.3618e+00 -7.2516e-01 -1.9872e-01\n",
      "  3.8641e-01  1.1044e-01 -1.8010e-01 -1.8099e-01  7.3046e-01 -2.0181e-01\n",
      "  4.6358e-01  1.0507e-01  6.3685e-01  6.9819e-01 -1.6141e-01 -6.1782e-01\n",
      " -3.7200e-01  3.4649e-01 -9.1107e-03  3.6265e-01  4.7032e-01  5.5006e-01\n",
      "  5.8578e-02 -6.3974e-02 -1.7195e-01 -8.2257e-02  3.8306e-02 -5.1057e-01\n",
      "  4.6162e-01 -4.9327e-01 -2.5698e-01  1.2622e-01 -4.2338e-01  1.5756e-01\n",
      "  2.3992e-01 -6.0445e-02 -2.0881e-01 -2.2697e-01 -3.5150e-01  9.8038e-01\n",
      " -3.1027e-01 -1.9599e-01 -2.3820e-02  2.4809e-01 -1.5286e-02  5.1643e-01\n",
      "  5.1180e-02 -2.4916e-01 -3.2044e-01  8.8840e-01  3.8730e-01  1.7669e-02\n",
      " -3.0996e-01 -1.8373e-01 -4.4646e-01  5.6889e-01  8.9490e-01 -3.8131e-01\n",
      "  1.0496e-01 -4.7026e-01  4.1761e-01 -1.3598e-01 -2.2293e-01 -4.3728e-01\n",
      " -7.4945e-01  4.4248e-02 -4.3045e-01 -4.8376e-01 -1.6265e-01  1.0318e-03\n",
      " -3.2195e-01 -9.4401e-01  3.5489e-01  6.2188e-02 -4.2378e-01 -3.7526e-01\n",
      " -9.3049e-01 -2.9614e-01 -1.4092e-01  1.8747e-01  1.8430e-01  3.7539e-01\n",
      "  2.1765e-02 -5.5020e-02  3.6201e-01 -5.1356e-02 -6.9036e-01 -4.3650e-01\n",
      " -6.0260e-01 -4.0345e-01  2.8776e-01 -7.8038e-02 -3.6173e-01 -3.4004e-01\n",
      " -2.2281e-01  5.6848e-01  1.6719e-01  2.6765e-01  3.9929e-01 -4.3413e-01\n",
      "  2.8054e-01 -1.6771e-01  3.2949e-01  8.2134e-02  4.4907e-01 -8.9879e-02\n",
      " -3.2885e-01  9.7572e-02  1.0701e-01 -3.8733e-01 -2.0622e-01 -3.0952e-01\n",
      "  7.7995e-01  9.2140e-02  4.6749e-01 -2.3249e-01  4.9351e-02  1.4282e-01\n",
      "  3.4758e-01  1.6618e-01 -3.6362e-01  7.5526e-02 -5.8117e-01 -1.3737e-01\n",
      " -6.2803e-01 -6.9712e-01 -5.8073e-01 -1.9061e-01 -1.2428e-02  4.6013e-01\n",
      " -1.0613e+00 -3.0966e-02  6.0271e-01 -1.2220e-01  2.8821e-01  2.4344e-01\n",
      " -5.5669e-01  1.0682e+00  1.0306e-03 -4.1243e-02  1.8721e-01 -2.8439e-01\n",
      " -4.8458e-01  3.5518e-01  3.7014e-02 -3.7345e-01 -2.1644e-01  5.3836e-01\n",
      " -1.7874e-01  3.8449e-01 -4.6711e-01 -4.1724e-02  2.8184e-01  1.1547e-01\n",
      " -5.1742e-01 -1.3962e+00  3.4417e-01  5.2339e-01  1.0341e+00  5.5010e-01\n",
      " -4.0277e-01 -7.0259e-01  5.2108e-01  4.2100e-01 -3.2766e-01  1.6396e-01\n",
      " -2.8539e-01  6.7881e-01 -1.3425e-03  1.9496e-01 -2.8886e-01  3.7637e-01\n",
      "  1.3091e-01  2.7179e-01  3.0134e-01  1.9964e-01 -2.7050e-01 -5.6043e-01\n",
      " -5.4140e-01 -6.7232e-01  3.0939e-01  4.3839e-01 -3.7351e-02  1.1932e-01\n",
      "  2.0856e-02 -2.4434e-01 -2.2094e-02 -9.2085e-01 -1.0949e-01 -3.8317e-01\n",
      "  1.3824e-01  6.0450e-01 -4.5901e-01  8.0913e-02 -1.6925e-01 -2.1354e-01]\n"
     ]
    }
   ],
   "source": [
    "# Process some text\n",
    "doc = nlp(\"Virat Kohli becomes fastest Indian cricketer to score 20000 international runs.\")\n",
    "\n",
    "# Access the vector via the token.vector attribute\n",
    "print(doc[0].vector) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### I. Token Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.40250298\n"
     ]
    }
   ],
   "source": [
    "doc1 = nlp(\"We are learning SpaCy\")\n",
    "doc2 = nlp(\"SpaCy is interesting\")\n",
    "\n",
    "token1 = doc1[2]\n",
    "token2 = doc2[2]\n",
    "\n",
    "print(token1.similarity(token2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### II. Span Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.42746237\n"
     ]
    }
   ],
   "source": [
    "doc1 = nlp(\"We are learning SpaCy\")\n",
    "doc2 = nlp(\"SpaCy is interesting\")\n",
    "\n",
    "span1 = doc1[2:]\n",
    "span2 = doc2[:]\n",
    "\n",
    "print(span1.similarity(span2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### III. Doc Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6511438662010053\n"
     ]
    }
   ],
   "source": [
    "doc1 = nlp(\"We are learning SpaCy\")\n",
    "doc2 = nlp(\"SpaCy is interesting\")\n",
    "\n",
    "print(doc1.similarity(doc2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VECTORIZATION\n",
    "\n",
    "Once the bag of words is created it needs to be encoded as integers or floating point values to be used as an input to a machine learning algorithm. This is called feature extraction (or vectorization).\n",
    "\n",
    "Let us understand Vectorization with a small example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = ['This is the first document.', \n",
    "        'This is the second second document.', \n",
    "        'And the third one.', \n",
    "        'Is this the first document?']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I. CountVectorizer\n",
    "\n",
    "The CountVectorizer provides a simple way to tokenize a collection of text documents, build a vocabulary of known words and create a document- token matrix.\n",
    "\n",
    "Let's use CountVectorizer from sklearn and create an instance of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# create the transform\n",
    "count_vectorizer = CountVectorizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Call the fit() function in order to learn a vocabulary from one or more documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'this': 8, 'is': 3, 'the': 6, 'first': 2, 'document': 1, 'second': 5, 'and': 0, 'third': 7, 'one': 4}\n"
     ]
    }
   ],
   "source": [
    "# tokenize and build vocab\n",
    "count_vectorizer.fit(text)\n",
    "\n",
    "# summarize\n",
    "print(count_vectorizer.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Call the transform() function on one or more documents as needed to encode each as a vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode document\n",
    "count_matrix = count_vectorizer.transform(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An encoded vector is returned with a length of the entire vocabulary and an integer count for the number of times each word appeared in the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 9)\n"
     ]
    }
   ],
   "source": [
    "# summarize encoded vector\n",
    "print(count_matrix.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because these vectors will contain a lot of zeros, we call them sparse. \n",
    "\n",
    "Python provides an efficient way of handling sparse vectors in the scipy.sparse package. \n",
    "\n",
    "The vectors returned from a call to transform() will be sparse vectors, and you can transform them back to numpy arrays to look and understand what is going on by calling the toarray() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'scipy.sparse.csr.csr_matrix'>\n",
      "\n",
      "[[0 1 1 1 0 0 1 0 1]\n",
      " [0 1 0 1 0 2 1 0 1]\n",
      " [1 0 0 0 1 0 1 1 0]\n",
      " [0 1 1 1 0 0 1 0 1]]\n"
     ]
    }
   ],
   "source": [
    "print(type(count_matrix))\n",
    "print()\n",
    "print(count_matrix.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The columns are all the word tokens (without punctuation) in sorted order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third', 'this']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>and</th>\n",
       "      <th>document</th>\n",
       "      <th>first</th>\n",
       "      <th>is</th>\n",
       "      <th>one</th>\n",
       "      <th>second</th>\n",
       "      <th>the</th>\n",
       "      <th>third</th>\n",
       "      <th>this</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   and  document  first  is  one  second  the  third  this\n",
       "0    0         1      1   1    0       0    1      0     1\n",
       "1    0         1      0   1    0       2    1      0     1\n",
       "2    1         0      0   0    1       0    1      1     0\n",
       "3    0         1      1   1    0       0    1      0     1"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(count_vectorizer.get_feature_names())\n",
    "import pandas as pd\n",
    "\n",
    "pd.DataFrame(count_matrix.toarray(), columns=count_vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### One can see that with CountVectorizer, all words were made lowercase by default and that the punctuation was ignored."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bi-gram CountVectorizer\n",
    "\n",
    "Note that in the previous corpus, the first and the last documents have exactly the same words hence are encoded in equal vectors. \n",
    "\n",
    "We lost the information that the last document is an interrogative form. \n",
    "\n",
    "To preserve some of the local ordering information we can extract 2-grams of words in addition to the 1-grams (individual words):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0],\n",
       "       [0, 0, 1, 0, 0, 1, 1, 0, 0, 2, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0],\n",
       "       [1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0],\n",
       "       [0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1]],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram_count_vectorizer = CountVectorizer(ngram_range=(1, 2))\n",
    "\n",
    "bigram_count_vectorizer.fit_transform(text).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['and', 'and the', 'document', 'first', 'first document', 'is', 'is the', 'is this', 'one', 'second', 'second document', 'second second', 'the', 'the first', 'the second', 'the third', 'third', 'third one', 'this', 'this is', 'this the']\n"
     ]
    }
   ],
   "source": [
    "print(bigram_count_vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II. TfidfVectorizer\n",
    "\n",
    "Word counts are a good starting point, but are very basic. One issue with simple counts is that longer document will have more imapct than small documents. An alternative is to calculate word frequencies, and by far the most popular method is called TF-IDF. This is an acronym than stands for â€œTerm Frequency â€“ Inverse Documentâ€.\n",
    "\n",
    "- Term Frequency: This summarizes how often a given term appears within a document.  \n",
    "- Inverse Document Frequency: This downscales terms that appear a lot across documents.\n",
    "\n",
    "![](img/tfidf.png)\n",
    "\n",
    "#### TF-IDF are word frequency scores that try to highlight words that are more interesting, e.g. frequent in a document but not across documents.\n",
    "\n",
    "The TfidfVectorizer will tokenize documents, learn the vocabulary and inverse document frequency weightings, and allow you to encode new documents. \n",
    "\n",
    "It is interesting to know that their are many variants of calculating tf and idf\n",
    "\n",
    "![](img/tfidf_formulas.png)\n",
    "\n",
    "Let's use TfidfVectorizer from sklearn and create an instance of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# create the transform\n",
    "tfidf_vectorizer = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Call the fit() function in order to learn a vocabulary from one or more documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'this': 8, 'is': 3, 'the': 6, 'first': 2, 'document': 1, 'second': 5, 'and': 0, 'third': 7, 'one': 4}\n"
     ]
    }
   ],
   "source": [
    "# tokenize and build vocab\n",
    "tfidf_vectorizer.fit(text)\n",
    "\n",
    "# summarize\n",
    "print(tfidf_vectorizer.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Call the transform() function on one or more documents as needed to encode each as a vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode document\n",
    "tfidf_matrix = tfidf_vectorizer.transform(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An encoded vector is returned with a length of the entire vocabulary and an integer count for the number of times each word appeared in the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.43877674 0.54197657 0.43877674 0.         0.\n",
      "  0.35872874 0.         0.43877674]\n",
      " [0.         0.27230147 0.         0.27230147 0.         0.85322574\n",
      "  0.22262429 0.         0.27230147]\n",
      " [0.55280532 0.         0.         0.         0.55280532 0.\n",
      "  0.28847675 0.55280532 0.        ]\n",
      " [0.         0.43877674 0.54197657 0.43877674 0.         0.\n",
      "  0.35872874 0.         0.43877674]]\n"
     ]
    }
   ],
   "source": [
    "print(tfidf_matrix.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The columns are all the word tokens (without punctuation) in sorted order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third', 'this']\n"
     ]
    }
   ],
   "source": [
    "print(tfidf_vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Web Scraping Example using Beautiful Soup\n",
    "\n",
    "Lastly, let's see a small web scrapping example using library BeautifulSoup to extract text data from a web page. \n",
    "\n",
    "You will have to install to libraries \n",
    "- requests, and\n",
    "- beautifulsoup4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "! pip install requests\n",
    "\n",
    "! pip install beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'     F.B.I. Agent Peter Strzok, Who Criticized Trump in Texts, Is Fired - The New York Times                                                                            SectionsSEARCHSkip to contentSkip to site indexPoliticsLog InLog InTodayâ€™s PaperPolitics|F.B.I. Agent Peter Strzok, Who Criticized Trump in Texts, Is FiredAdvertisementSupported byF.B.I. Agent Peter Strzok, Who Criticized Trump in Texts, Is FiredImagePeter Strzok, a top F.B.I. counterintelligence agent who was taken off the special counsel investigation after his disparaging texts about President Trump were uncovered, was fired.CreditCreditT.J. Kirkpatrick for The New York TimesBy Adam Goldman and Michael S. SchmidtAug. 13, 2018WASHINGTON â€” Peter Strzok, the F.B.I. senior counterintelligence agent who disparaged President Trump in inflammatory text messages and helped oversee the Hillary Clinton email and Russia investigations, has been fired for violating bureau policies, Mr. Strzokâ€™s lawyer said Monday.Mr. Trump and his'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import re\n",
    "\n",
    "def url_to_string(url):\n",
    "    \n",
    "    res = requests.get(url)\n",
    "    html = res.text\n",
    "    soup = BeautifulSoup(html, 'html5lib')\n",
    "    \n",
    "    # remove all javascript and stylesheet code\n",
    "    for script in soup([\"script\", \"style\", 'aside']):\n",
    "        script.extract()\n",
    "    \n",
    "    return \" \".join(re.split(r'[\\n\\t]+', soup.get_text()))\n",
    "\n",
    "text = url_to_string('https://www.nytimes.com/2018/08/13/us/politics/peter-strzok-fired-fbi.html?\\\n",
    "                        hp&action=click&pgtype=Homepage&clickSource=story-heading&\\\n",
    "                        module=first-column-region&region=top-news&WT.nav=top-news')\n",
    "\n",
    "text = text[:1001]\n",
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "- https://spacy.io/usage\n",
    "\n",
    "### Appendix\n",
    "\n",
    "I. Annotation tool (also developed by Explosion.AI)\n",
    "\n",
    "- https://prodi.gy/demo?view_id=ner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
